{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: AppCurateGEO \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def process_gse(gse_id, super_series=None):\n",
    "    # If SuperSeries is not set, use the current GSE\n",
    "    if not super_series:\n",
    "        super_series = gse_id\n",
    "\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse_id}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    desired_fields = [\n",
    "        \"Title\",\n",
    "        \"Summary\",\n",
    "        \"Overall design\",\n",
    "        \"Contact name\",\n",
    "        \"E-mail(s)\",\n",
    "        \"Phone\",\n",
    "        \"Organization name\",\n",
    "        \"Department\",\n",
    "        \"Lab\",\n",
    "        \"City\",\n",
    "        \"State/province\",\n",
    "        \"Country\",\n",
    "    ]\n",
    "\n",
    "    data = {}\n",
    "    rows = soup.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == 2:\n",
    "            label = cols[0].get_text(strip=True)\n",
    "            value = cols[1].get_text(strip=True)\n",
    "            if label in desired_fields:\n",
    "                data[label] = value\n",
    "\n",
    "    # Get platforms & samples from SOFT\n",
    "    soft_url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse_id}&format=soft\"\n",
    "    soft_response = requests.get(soft_url)\n",
    "    soft_text = soft_response.text\n",
    "\n",
    "    platforms = set(re.findall(r\"(GPL\\d+)\", soft_text))\n",
    "    platforms_str = \", \".join(sorted(platforms))\n",
    "    samples = set(re.findall(r\"(GSM\\d+)\", soft_text))\n",
    "    num_samples = len(samples)\n",
    "\n",
    "    data[\"Platforms\"] = platforms_str\n",
    "    data[\"Samples\"] = num_samples\n",
    "    data[\"Series\"] = gse_id\n",
    "    data[\"SuperSeries\"] = super_series\n",
    "\n",
    "    supp_data = []\n",
    "\n",
    "    # -------------------------\n",
    "    # Case 1: Selenium \"(custom)\" scraping if link exists\n",
    "    # -------------------------\n",
    "    if soup.find(\"a\", string=\"(custom)\"):\n",
    "        driver = webdriver.Chrome()\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "\n",
    "            custom_link = wait.until(\n",
    "                EC.element_to_be_clickable((By.LINK_TEXT, \"(custom)\"))\n",
    "            )\n",
    "            custom_link.click()\n",
    "\n",
    "            wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//input[@type='checkbox']\")))\n",
    "            checkboxes = driver.find_elements(By.XPATH, \"//input[@type='checkbox']\")\n",
    "\n",
    "            files = []\n",
    "            for checkbox in checkboxes:\n",
    "                parent_text = checkbox.find_element(By.XPATH, \"./..\").text\n",
    "                files.append(parent_text)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "        files = files[:-1] # exclude last item which is \"(all files)\"\n",
    "\n",
    "        # Exclude \"(all files)\" if present at end\n",
    "        files = [f for f in files if f.lower() != \"(all files)\"]\n",
    "\n",
    "        if files:\n",
    "            for f in files:\n",
    "                row_dict = data.copy()\n",
    "                row_dict[\"Custom File\"] = f\n",
    "                supp_data.append(row_dict)\n",
    "        else:\n",
    "            supp_data.append(data)\n",
    "\n",
    "    # -------------------------\n",
    "    # Case 2: Default parsing (original code)\n",
    "    # -------------------------\n",
    "    else:\n",
    "        tables = soup.find_all('table')\n",
    "        supp_table = None\n",
    "        for table in tables[::-1]:\n",
    "            header_row = table.find('tr')\n",
    "            if not header_row:\n",
    "                continue\n",
    "            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['td', 'th'])]\n",
    "            if (\n",
    "                \"Supplementary file\" in headers\n",
    "                and \"Size\" in headers\n",
    "                and \"File type/resource\" in headers\n",
    "            ):\n",
    "                supp_table = table\n",
    "                break\n",
    "\n",
    "        if supp_table:\n",
    "            rows = supp_table.find_all('tr')[1:]  # skip header\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 4:\n",
    "                    file_name = cells[0].get_text(strip=True)\n",
    "                    size = cells[1].get_text(strip=True)\n",
    "                    file_type = cells[3].get_text(strip=True)\n",
    "                    row_dict = data.copy()\n",
    "                    row_dict.update({\n",
    "                        \"Supplementary file\": file_name,\n",
    "                        \"Size\": size,\n",
    "                        \"File type/resource\": file_type\n",
    "                    })\n",
    "                    supp_data.append(row_dict)\n",
    "        else:\n",
    "            # If no supplementary table, at least record metadata once\n",
    "            supp_data.append(data)\n",
    "\n",
    "    return supp_data\n",
    "\n",
    "# This script needs to be on my local machine NOT in OneDrive \n",
    "# local at /Users/mmarcao/Documents/GEO_app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output dir\n",
    "dir_base = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting GSEs from manual query on GEO DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gds_file_path = os.path.join(dir_base, \"gds_result.txt\")\n",
    "proximity_window = 10  # Define how close the GSE numeric IDs should be to belong in the same group\n",
    "\n",
    "# Step 1: Read the file content\n",
    "with open(gds_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 2: Extract all GSE accession IDs\n",
    "gse_ids = gse_ids = re.findall(r'GSE(\\d+)\\b', text)  # Just numeric parts as strings\n",
    "gse_ids = sorted(set(gse_ids), key=int)  # Sort numerically and unique\n",
    "\n",
    "# Step 3: Cluster GSEs by numeric proximity\n",
    "clusters = []\n",
    "cluster_index = 0\n",
    "current_cluster = []\n",
    "prev_num = None\n",
    "\n",
    "for gse_num_str in gse_ids:\n",
    "    gse_num = int(gse_num_str)\n",
    "    if prev_num is None:\n",
    "        # start first cluster\n",
    "        current_cluster = [gse_num]\n",
    "        cluster_index = 1\n",
    "    else:\n",
    "        # check if current gse_num is \"close\" to prev_num to be in the same cluster\n",
    "        if gse_num - prev_num <= proximity_window:\n",
    "            current_cluster.append(gse_num)\n",
    "        else:\n",
    "            # finalize current cluster, start new\n",
    "            clusters.append((cluster_index, current_cluster))\n",
    "            cluster_index += 1\n",
    "            current_cluster = [gse_num]\n",
    "    prev_num = gse_num\n",
    "\n",
    "# Add last cluster if not empty\n",
    "if current_cluster:\n",
    "    clusters.append((cluster_index, current_cluster))\n",
    "\n",
    "# Step 4: Create a mapping from GSE number to cluster index\n",
    "gse_to_cluster = {}\n",
    "for cluster_id, gse_list in clusters:\n",
    "    for val in gse_list:\n",
    "        gse_to_cluster[val] = cluster_id\n",
    "\n",
    "# Step 5: Build DataFrame with columns 'GSE' and 'Cluster'\n",
    "data = []\n",
    "for gse_num_str in gse_ids:\n",
    "    gse_num = int(gse_num_str)\n",
    "    cluster_id = gse_to_cluster.get(gse_num, None)\n",
    "    gse_code = f\"GSE{gse_num_str}\"\n",
    "    data.append({\"GSE\": gse_code, \"Cluster\": f\"Cluster{cluster_id}\"})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates(subset=['GSE']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Optional: save to CSV\n",
    "df.to_csv(dir_base + \"/gds_processed.csv\", index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping each GSE into a dataframe output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of GSE IDs to process\n",
    "gse_list = df['GSE']  # replace with your GSEs\n",
    "\n",
    "all_data = []\n",
    "for gse in gse_list:\n",
    "    print(f\"Processing {gse} ...\")\n",
    "    all_data.extend(process_gse(gse))\n",
    "\n",
    "df_combined = pd.DataFrame(all_data)\n",
    "\n",
    "# Save all results to CSV\n",
    "os.makedirs(dir_base, exist_ok=True)\n",
    "combined_path = os.path.join(dir_base, \"geo_webscrap.csv\")\n",
    "df_combined.to_csv(combined_path, index=False)\n",
    "\n",
    "print(df_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIDS2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
