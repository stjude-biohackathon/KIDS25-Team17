{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: AppCurateGEO \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def process_gse(gse_id, super_series=None):\n",
    "    # If SuperSeries is not set, use the current GSE\n",
    "    if not super_series:\n",
    "        super_series = gse_id\n",
    "\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse_id}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    desired_fields = [\n",
    "        \"Title\",\n",
    "        \"Summary\",\n",
    "        \"Overall design\",\n",
    "        \"Contact name\",\n",
    "        \"E-mail(s)\",\n",
    "        \"Phone\",\n",
    "        \"Organization name\",\n",
    "        \"Department\",\n",
    "        \"Lab\",\n",
    "        \"City\",\n",
    "        \"State/province\",\n",
    "        \"Country\",\n",
    "    ]\n",
    "\n",
    "    data = {}\n",
    "    rows = soup.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == 2:\n",
    "            label = cols[0].get_text(strip=True)\n",
    "            value = cols[1].get_text(strip=True)\n",
    "            if label in desired_fields:\n",
    "                data[label] = value\n",
    "\n",
    "    # Get platforms & samples from SOFT\n",
    "    soft_url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse_id}&format=soft\"\n",
    "    soft_response = requests.get(soft_url)\n",
    "    soft_text = soft_response.text\n",
    "\n",
    "    platforms = set(re.findall(r\"(GPL\\d+)\", soft_text))\n",
    "    platforms_str = \", \".join(sorted(platforms))\n",
    "    samples = set(re.findall(r\"(GSM\\d+)\", soft_text))\n",
    "    num_samples = len(samples)\n",
    "\n",
    "    data[\"Platforms\"] = platforms_str\n",
    "    data[\"Samples\"] = num_samples\n",
    "    data[\"Series\"] = gse_id\n",
    "    data[\"SuperSeries\"] = super_series\n",
    "\n",
    "    supp_data = []\n",
    "\n",
    "    # -------------------------\n",
    "    # Case 1: Selenium \"(custom)\" scraping if link exists\n",
    "    # -------------------------\n",
    "    if soup.find(\"a\", string=\"(custom)\"):\n",
    "        driver = webdriver.Chrome()\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "\n",
    "            custom_link = wait.until(\n",
    "                EC.element_to_be_clickable((By.LINK_TEXT, \"(custom)\"))\n",
    "            )\n",
    "            custom_link.click()\n",
    "\n",
    "            wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//input[@type='checkbox']\")))\n",
    "            checkboxes = driver.find_elements(By.XPATH, \"//input[@type='checkbox']\")\n",
    "\n",
    "            files = []\n",
    "            for checkbox in checkboxes:\n",
    "                parent_text = checkbox.find_element(By.XPATH, \"./..\").text\n",
    "                files.append(parent_text)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "        files = files[:-1] # exclude last item which is \"(all files)\"\n",
    "\n",
    "        # Exclude \"(all files)\" if present at end\n",
    "        files = [f for f in files if f.lower() != \"(all files)\"]\n",
    "\n",
    "        if files:\n",
    "            for f in files:\n",
    "                row_dict = data.copy()\n",
    "                row_dict[\"Custom File\"] = f\n",
    "                supp_data.append(row_dict)\n",
    "        else:\n",
    "            supp_data.append(data)\n",
    "\n",
    "    # -------------------------\n",
    "    # Case 2: Default parsing (original code)\n",
    "    # -------------------------\n",
    "    else:\n",
    "        tables = soup.find_all('table')\n",
    "        supp_table = None\n",
    "        for table in tables[::-1]:\n",
    "            header_row = table.find('tr')\n",
    "            if not header_row:\n",
    "                continue\n",
    "            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['td', 'th'])]\n",
    "            if (\n",
    "                \"Supplementary file\" in headers\n",
    "                and \"Size\" in headers\n",
    "                and \"File type/resource\" in headers\n",
    "            ):\n",
    "                supp_table = table\n",
    "                break\n",
    "\n",
    "        if supp_table:\n",
    "            rows = supp_table.find_all('tr')[1:]  # skip header\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 4:\n",
    "                    file_name = cells[0].get_text(strip=True)\n",
    "                    size = cells[1].get_text(strip=True)\n",
    "                    file_type = cells[3].get_text(strip=True)\n",
    "                    row_dict = data.copy()\n",
    "                    row_dict.update({\n",
    "                        \"Supplementary file\": file_name,\n",
    "                        \"Size\": size,\n",
    "                        \"File type/resource\": file_type\n",
    "                    })\n",
    "                    supp_data.append(row_dict)\n",
    "        else:\n",
    "            # If no supplementary table, at least record metadata once\n",
    "            supp_data.append(data)\n",
    "\n",
    "    return supp_data\n",
    "\n",
    "# This script needs to be on my local machine NOT in OneDrive \n",
    "# local at /Users/mmarcao/Documents/GEO_app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Search\n",
    "query = 'pediatric glioma spatial transcriptomics'\n",
    "dir_base = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GSE271936', 'GSE268577', 'GSE194329']\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.ncbi.nlm.nih.gov/geo/\") #open webpage\n",
    "\n",
    "search_bar = driver.find_element(by=By.CLASS_NAME, value=\"jig-ncbiclearbutton\") #identify search bar\n",
    "search_bar.send_keys(query) #enter your query\n",
    "\n",
    "#see search results\n",
    "search_button = driver.find_element(by=By.ID, value=\"setacc\")\n",
    "search_button.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "search_link = wait.until(EC.element_to_be_clickable(\n",
    "    (By.XPATH, \"//p[contains(., 'results for') and contains(., 'DataSets')]//a\")\n",
    "))\n",
    "search_link.click()\n",
    "\n",
    "gse_list=[]\n",
    "while True:\n",
    "    try:\n",
    "        #check to make sure you are not on the last page\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@title='Next page of results']\")\n",
    "    except:\n",
    "        #extract GSE ids on last page and exit\n",
    "        for element in driver.find_elements(By.XPATH, \"//*[contains(text(), 'GSE')]\"):\n",
    "            gse_list.append(element.text)\n",
    "        break\n",
    "    else:\n",
    "        #extract GSE ids and put in list\n",
    "        for element in driver.find_elements(By.XPATH, \"//*[contains(text(), 'GSE')]\"):\n",
    "            gse_list.append(element.text)\n",
    "        #navigate to the next page\n",
    "        next_button = driver.find_element(By.XPATH, \n",
    "        \"//a[@title='Next page of results']\")\n",
    "        next_button.click()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(gse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping each GSE into a dataframe output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GSE271936 ...\n",
      "Processing GSE268577 ...\n",
      "Processing GSE194329 ...\n",
      "                                                Title  \\\n",
      "0   Immune and tumor cell landscape in pediatric h...   \n",
      "1   Immune and tumor cell landscape in pediatric h...   \n",
      "2   Immune and tumor cell landscape in pediatric h...   \n",
      "3   Immune and tumor cell landscape in pediatric h...   \n",
      "4   Immune and tumor cell landscape in pediatric h...   \n",
      "..                                                ...   \n",
      "57  Spatial multi-omics reveals vulnerabilities of...   \n",
      "58  Spatial multi-omics reveals vulnerabilities of...   \n",
      "59  Spatial multi-omics reveals vulnerabilities of...   \n",
      "60  Spatial multi-omics reveals vulnerabilities of...   \n",
      "61  Spatial multi-omics reveals vulnerabilities of...   \n",
      "\n",
      "                                              Summary  \\\n",
      "0   Single cell RNA-seq profiling of tumor, myeloi...   \n",
      "1   Single cell RNA-seq profiling of tumor, myeloi...   \n",
      "2   Single cell RNA-seq profiling of tumor, myeloi...   \n",
      "3   Single cell RNA-seq profiling of tumor, myeloi...   \n",
      "4   Single cell RNA-seq profiling of tumor, myeloi...   \n",
      "..                                                ...   \n",
      "57  We collected five DMG, five GBM (including two...   \n",
      "58  We collected five DMG, five GBM (including two...   \n",
      "59  We collected five DMG, five GBM (including two...   \n",
      "60  We collected five DMG, five GBM (including two...   \n",
      "61  We collected five DMG, five GBM (including two...   \n",
      "\n",
      "                                       Overall design  \\\n",
      "0   We used Xenium Analyzer Spatial Transcriptomic...   \n",
      "1   We used Xenium Analyzer Spatial Transcriptomic...   \n",
      "2   We used Xenium Analyzer Spatial Transcriptomic...   \n",
      "3   We used Xenium Analyzer Spatial Transcriptomic...   \n",
      "4   We used Xenium Analyzer Spatial Transcriptomic...   \n",
      "..                                                ...   \n",
      "57  Here, we established a spatial atlas of H3K27M...   \n",
      "58  Here, we established a spatial atlas of H3K27M...   \n",
      "59  Here, we established a spatial atlas of H3K27M...   \n",
      "60  Here, we established a spatial atlas of H3K27M...   \n",
      "61  Here, we established a spatial atlas of H3K27M...   \n",
      "\n",
      "                               Contact name  \\\n",
      "0   Carlos Alberto Oliveira de Biagi Junior   \n",
      "1   Carlos Alberto Oliveira de Biagi Junior   \n",
      "2   Carlos Alberto Oliveira de Biagi Junior   \n",
      "3   Carlos Alberto Oliveira de Biagi Junior   \n",
      "4   Carlos Alberto Oliveira de Biagi Junior   \n",
      "..                                      ...   \n",
      "57                                Yuan Wang   \n",
      "58                                Yuan Wang   \n",
      "59                                Yuan Wang   \n",
      "60                                Yuan Wang   \n",
      "61                                Yuan Wang   \n",
      "\n",
      "                               E-mail(s)             Organization name  \\\n",
      "0   carlosa_biagijunior@dfci.harvard.edu  Dana Farber Cancer Institute   \n",
      "1   carlosa_biagijunior@dfci.harvard.edu  Dana Farber Cancer Institute   \n",
      "2   carlosa_biagijunior@dfci.harvard.edu  Dana Farber Cancer Institute   \n",
      "3   carlosa_biagijunior@dfci.harvard.edu  Dana Farber Cancer Institute   \n",
      "4   carlosa_biagijunior@dfci.harvard.edu  Dana Farber Cancer Institute   \n",
      "..                                   ...                           ...   \n",
      "57                   wangyuan@scu.edu.cn            Sichuan university   \n",
      "58                   wangyuan@scu.edu.cn            Sichuan university   \n",
      "59                   wangyuan@scu.edu.cn            Sichuan university   \n",
      "60                   wangyuan@scu.edu.cn            Sichuan university   \n",
      "61                   wangyuan@scu.edu.cn            Sichuan university   \n",
      "\n",
      "            Department              Lab     City State/province Country  \\\n",
      "0   Pediatric Oncology  Mariella Filbin   Boston             MA     USA   \n",
      "1   Pediatric Oncology  Mariella Filbin   Boston             MA     USA   \n",
      "2   Pediatric Oncology  Mariella Filbin   Boston             MA     USA   \n",
      "3   Pediatric Oncology  Mariella Filbin   Boston             MA     USA   \n",
      "4   Pediatric Oncology  Mariella Filbin   Boston             MA     USA   \n",
      "..                 ...              ...      ...            ...     ...   \n",
      "57                 NaN              NaN  Chengdu        Sichuan   China   \n",
      "58                 NaN              NaN  Chengdu        Sichuan   China   \n",
      "59                 NaN              NaN  Chengdu        Sichuan   China   \n",
      "60                 NaN              NaN  Chengdu        Sichuan   China   \n",
      "61                 NaN              NaN  Chengdu        Sichuan   China   \n",
      "\n",
      "   Platforms  Samples     Series SuperSeries  \\\n",
      "0   GPL33762        4  GSE271936   GSE271936   \n",
      "1   GPL33762        4  GSE271936   GSE271936   \n",
      "2   GPL33762        4  GSE271936   GSE271936   \n",
      "3   GPL33762        4  GSE271936   GSE271936   \n",
      "4   GPL33762        4  GSE271936   GSE271936   \n",
      "..       ...      ...        ...         ...   \n",
      "57  GPL11154       11  GSE194329   GSE194329   \n",
      "58  GPL11154       11  GSE194329   GSE194329   \n",
      "59  GPL11154       11  GSE194329   GSE194329   \n",
      "60  GPL11154       11  GSE194329   GSE194329   \n",
      "61  GPL11154       11  GSE194329   GSE194329   \n",
      "\n",
      "                                          Custom File  \n",
      "0       GSM8389375_H3K27M_P6_Region_1-barcodes.tsv.gz  \n",
      "1       GSM8389375_H3K27M_P6_Region_1-features.tsv.gz  \n",
      "2         GSM8389375_H3K27M_P6_Region_1-matrix.mtx.gz  \n",
      "3   GSM8389375_H3K27M_P6_Region_1-morphology.ome.t...  \n",
      "4    GSM8389375_H3K27M_P6_Region_1-transcripts.csv.gz  \n",
      "..                                                ...  \n",
      "57             GSM5833534_GBM2_spaceranger_out.tar.gz  \n",
      "58             GSM5833535_GBM3_spaceranger_out.tar.gz  \n",
      "59             GSM5833536_GBM4_spaceranger_out.tar.gz  \n",
      "60           GSM5833537_GBM5_1_spaceranger_out.tar.gz  \n",
      "61           GSM5833538_GBM5_2_spaceranger_out.tar.gz  \n",
      "\n",
      "[62 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of GSE IDs to process\n",
    "all_data = []\n",
    "for gse in gse_list:\n",
    "    print(f\"Processing {gse} ...\")\n",
    "    all_data.extend(process_gse(gse))\n",
    "\n",
    "df_combined = pd.DataFrame(all_data)\n",
    "\n",
    "# Save all results to CSV\n",
    "os.makedirs(dir_base, exist_ok=True)\n",
    "combined_path = os.path.join(dir_base, \"geo_webscrap.csv\")\n",
    "df_combined.to_csv(combined_path, index=False)\n",
    "\n",
    "print(df_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIDS2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
